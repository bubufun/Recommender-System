{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = ['BASKET_COMM','BASKET_SUM_COMM','BASKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made folder: ./models/BASKET_COMM/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\202009-da02008\\pycharmprojects\\w2v_dnn_embedding\\venv\\lib\\site-packages\\ipykernel_launcher.py:83: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./models/embedding_log/BASKET_COMM_embedding_log/model.ckpt\n",
      "made folder: ./models/BASKET_SUM_COMM/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\202009-da02008\\pycharmprojects\\w2v_dnn_embedding\\venv\\lib\\site-packages\\ipykernel_launcher.py:83: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./models/embedding_log/BASKET_SUM_COMM_embedding_log/model.ckpt\n",
      "made folder: ./models/BASKET/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\202009-da02008\\pycharmprojects\\w2v_dnn_embedding\\venv\\lib\\site-packages\\ipykernel_launcher.py:83: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: ./models/embedding_log/BASKET_embedding_log/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "data_root = os.path.abspath(os.getcwd())+ '/data/'\n",
    "\n",
    "# raw_data_path = data_root + '/raw/'\n",
    "\n",
    "processed_data_path = data_root + '/processed/'\n",
    "\n",
    "model_path = './models/'\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    print(\"made folder:\", model_path)\n",
    "\n",
    "if not os.path.exists('./models/embedding_log/'):\n",
    "    os.mkdir('./models/embedding_log/')\n",
    "\n",
    "for keyword in keyword_list:\n",
    "    \n",
    "    #Parameter Definition\n",
    "    \n",
    "    csv_path = processed_data_path + keyword + '.csv'\n",
    "    w2v_csv_path = processed_data_path + keyword + '_w2v' + '.csv'\n",
    "    model_name = keyword + 'w2v_model_all.md'\n",
    "    \n",
    "    #Train & Save W2V models \n",
    "    \n",
    "    pd_corpus = pd.read_csv(w2v_csv_path)\n",
    "    corpus = pd.DataFrame(pd_corpus.words.sample(frac=1).unique(),\n",
    "                   columns=['words'])\n",
    "    \n",
    "    corpus.words = corpus.words.apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "    model_keyword_path = model_path + keyword + '/'\n",
    "    \n",
    "    if not os.path.isdir(model_keyword_path):\n",
    "        os.makedirs(model_keyword_path)\n",
    "        print(\"made folder:\", model_keyword_path)\n",
    "    \n",
    "    model_path_cbow = model_keyword_path + model_name + '.' + 'cbow'\n",
    "    model_path_sg = model_keyword_path + model_name + '.' + 'sg'\n",
    "    \n",
    "    try:\n",
    "        model = Word2Vec(corpus.words)\n",
    "        model.save(model_path_cbow)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        model = Word2Vec(corpus.words, sg=1)\n",
    "        model.save(model_path_sg)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    #Embedding log created\n",
    "    \n",
    "    \n",
    "    model_dir = model_path\n",
    "    \n",
    "    log_dir = model_dir + 'embedding_log/' +keyword +'_embedding_log'  + '/'\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    \n",
    "    metadata_name = 'metadata.tsv'\n",
    "    \n",
    "    word2vec = Word2Vec.load(model_path_cbow)\n",
    "    \n",
    "    wv_dict = {'words':[], 'counts':[], 'wv':[]}\n",
    "    \n",
    "    for word in word2vec.wv.vocab.keys():\n",
    "        if word2vec.wv.vocab[word].count > 4:                      #Word count limit\n",
    "            wv_dict['words'].append(word)\n",
    "            wv_dict['counts'].append(word2vec.wv.vocab[word].count)\n",
    "            wv_dict['wv'].append(word2vec[word])\n",
    "        \n",
    "    pd_wv = pd.DataFrame(wv_dict)\n",
    "    \n",
    "    # write labels and count\n",
    "    embedding = np.empty((len(pd_wv), word2vec.vector_size), dtype=np.float32)\n",
    "    with open(os.path.join(log_dir, metadata_name), 'w') as f:\n",
    "        f.write('word' + '\\t' + 'vol_lv' + '\\t' + 'count' + '\\n')\n",
    "        for i, row in pd_wv.iterrows():\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(row.words, round(math.log(row.counts, 10)), row.counts))\n",
    "            embedding[i] = row.wv\n",
    "    \n",
    "    \n",
    "    embeddings_vectors = embedding\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    # Create some variables.\n",
    "    emb = tf.compat.v1.Variable(embeddings_vectors, name='word_embeddings')\n",
    "\n",
    "    # Add an op to initialize the variable.\n",
    "    init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "    # Later, launch the model, initialize the variables and save the\n",
    "    # variables to disk.\n",
    "    \n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, os.path.join(log_dir, \"model.ckpt\"))\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "        # Set up config\n",
    "        config = projector.ProjectorConfig()\n",
    "        # One can add multiple embeddings.\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = emb.name\n",
    "        # Link this tensor to its metadata file (e.g. labels).\n",
    "        embedding.metadata_path = metadata_name\n",
    "        # Saves a config file that TensorBoard will read during startup.\n",
    "        # projector.visualize_embeddings(log_dir, config)\n",
    "        \n",
    "    projector.visualize_embeddings(tf.compat.v1.summary.FileWriter(log_dir), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
